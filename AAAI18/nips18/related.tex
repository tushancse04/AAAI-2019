\subsection{Related Work}

Lifted inference methods exploiting exact symmetries have been explored in several earlier works in the context of exact inference (\cite{poole03,braz07,gogate&domingos11b,broeck&al11}) as well as approximate inference (\cite{singla&domingos08,niepert12,venugopal&gogate12}). Further, the idea of using approximate symmetries for inference has been used to develop general pre-processing methods such as using boolean factorization to generate a symmetric approximation of evidence~\cite{broeck&darwiche13}, and clustering domain objects to reduce the MLN size~\cite{venugopal&gogate14}. More recently, approximate symmetries have been used in specific inference algorithms, such as MAP inference~\cite{sarkhel&al15}, sampling-based inference~\cite{broeck&niepert15}, and linear programming~\cite{kersting&al17}. Kopp et al.~\cite{kopp&al15} used symmetry breaking techniques commonly used in the SAT community and applied it to lifted inference. More recently, Anand et al. ~\cite{anand&al16,anand&al17}proposed the use of non-count symmetries, and contextual symmetries in graphical models. Specifically, they used graph-based tools to detect automorphism groups, and infer symmetries from such groups. There has also been several previous works that have focussed on learning embeddings for relational data~\cite{bordes&al11}. However, to the best of our knowledge, ours is the first work to consider lifted inference using neural embeddings.